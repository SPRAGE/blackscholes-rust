name: Benchmark Regression Detection

on:
  pull_request:
    branches: [ main, master ]
    paths:
      - 'src/**'
      - 'benches/**'
      - 'Cargo.toml'
      - 'Cargo.lock'
  workflow_dispatch:  # Allow manual triggering

env:
  CARGO_TERM_COLOR: always
  RUSTFLAGS: "-C target-cpu=native"
  # Define performance regression thresholds (percentage)
  WARNING_THRESHOLD: 5
  FAILURE_THRESHOLD: 10

jobs:
  detect-regression:
    name: Detect Performance Regressions
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout PR branch
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # Fetch all history
      
      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable
        with:
          components: rustfmt, clippy
      
      - name: Cache cargo registry
        uses: Swatinem/rust-cache@v2
      
      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y gnuplot jq libfontconfig1-dev
          
          # Install custom benchmark comparison tool
          mkdir -p scripts/tools
          
          # Create a benchmark comparison tool to detect regressions
          cat > scripts/tools/compare_benchmarks.sh << 'EOF'
          #!/bin/bash
          set -e
          
          # Process benchmark results and detect regressions
          PR_RESULTS="$1"
          BASE_RESULTS="$2"
          WARNING_THRESHOLD="$3"
          FAILURE_THRESHOLD="$4"
          
          if [ ! -f "$PR_RESULTS" ] || [ ! -f "$BASE_RESULTS" ]; then
            echo "Error: Benchmark result files not found"
            echo "PR file: $PR_RESULTS (exists: $([ -f "$PR_RESULTS" ] && echo "yes" || echo "no"))"
            echo "Base file: $BASE_RESULTS (exists: $([ -f "$BASE_RESULTS" ] && echo "yes" || echo "no"))"
            exit 1
          fi
          
          # Extract benchmark times from JSON
          PR_TIME=$(jq '.mean.point_estimate' "$PR_RESULTS")
          BASE_TIME=$(jq '.mean.point_estimate' "$BASE_RESULTS")
          
          if [ -z "$PR_TIME" ] || [ -z "$BASE_TIME" ] || [ "$BASE_TIME" = "null" ] || [ "$PR_TIME" = "null" ]; then
            echo "Error: Could not extract benchmark times"
            echo "PR time value: $PR_TIME"
            echo "Base time value: $BASE_TIME"
            exit 1
          fi
          
          # Calculate regression percentage
          PERCENT_CHANGE=$(echo "scale=2; 100 * ($PR_TIME - $BASE_TIME) / $BASE_TIME" | bc)
          
          # Format outputs (remove decimals for comparison but keep for display)
          PERCENT_CHANGE_DISPLAY=$PERCENT_CHANGE
          PERCENT_CHANGE_INT=$(echo "$PERCENT_CHANGE" | awk '{print int($1)}')
          WARNING_THRESHOLD_INT=$(echo "$WARNING_THRESHOLD" | awk '{print int($1)}')
          FAILURE_THRESHOLD_INT=$(echo "$FAILURE_THRESHOLD" | awk '{print int($1)}')
          
          BENCH_NAME=$(basename $(dirname "$PR_RESULTS"))
          
          # Detect regression
          if [ "$PERCENT_CHANGE_INT" -ge "$FAILURE_THRESHOLD_INT" ]; then
            echo "::error::❌ CRITICAL REGRESSION in $BENCH_NAME: ${PERCENT_CHANGE_DISPLAY}% slower! (${BASE_TIME} ns → ${PR_TIME} ns)"
            exit 1
          elif [ "$PERCENT_CHANGE_INT" -ge "$WARNING_THRESHOLD_INT" ]; then
            echo "::warning::⚠️ POSSIBLE REGRESSION in $BENCH_NAME: ${PERCENT_CHANGE_DISPLAY}% slower (${BASE_TIME} ns → ${PR_TIME} ns)"
          elif [ "$PERCENT_CHANGE_INT" -le "-$WARNING_THRESHOLD_INT" ]; then
            echo "::notice::✅ IMPROVEMENT in $BENCH_NAME: ${PERCENT_CHANGE_DISPLAY}% faster (${BASE_TIME} ns → ${PR_TIME} ns)"
          else
            echo "✓ No significant change in $BENCH_NAME: ${PERCENT_CHANGE_DISPLAY}% (${BASE_TIME} ns → ${PR_TIME} ns)"
          fi
          
          # Return exit code 0 always to continue processing other benchmarks
          exit 0
          EOF
          
          chmod +x scripts/tools/compare_benchmarks.sh
      
      - name: Run PR branch benchmarks
        run: |
          # Create directory for results
          mkdir -p target/bench-comparison/pr
          
          # Run tests first
          cargo test
          
          # Run all benchmarks without any options
          cargo bench
          
          # Debug output - show what criterion files were created
          echo "=== PR Branch Benchmark Files ==="
          find target/criterion -type f -name "*.json" | sort
      
      - name: Checkout base branch
        run: |
          # Determine the base branch
          TARGET_BRANCH=${{ github.base_ref }}
          echo "Checking out base branch: $TARGET_BRANCH"
          git checkout origin/$TARGET_BRANCH
      
      - name: Run base branch benchmarks
        run: |
          # Create directory for results
          mkdir -p target/bench-comparison/base
          
          # Run tests first
          cargo test
          
          # Run same benchmarks on base branch
          cargo bench
          
          # Debug output - show what criterion files were created
          echo "=== Base Branch Benchmark Files ==="
          find target/criterion -type f -name "*.json" | sort
      
      - name: Extract and compare benchmark results
        run: |
          echo "Comparing benchmark results between PR and base branch..."
          echo "Warning threshold: $WARNING_THRESHOLD%, Failure threshold: $FAILURE_THRESHOLD%"
          
          # Find all benchmark JSON files
          PR_RESULTS=$(find target/criterion -name "new.json" | sort)
          
          # Debugging - show what files were found
          echo "Found $(echo "$PR_RESULTS" | wc -l) PR result files:"
          echo "$PR_RESULTS"
          
          if [ -z "$PR_RESULTS" ]; then
            echo "⚠️ No PR benchmark results found!"
            echo "Checking for any JSON files..."
            find target/criterion -name "*.json" | sort
            
            echo "Directory structure:"
            find target/criterion -type d | sort
          fi
          
          # Process each benchmark result
          SUCCESS=true
          FOUND_FILES=false
          
          for PR_FILE in $PR_RESULTS; do
            # Find corresponding base file
            RELATIVE_PATH=${PR_FILE#"target/criterion/"}
            BASE_FILE="target/criterion/${RELATIVE_PATH/new/base}"
            
            echo "Checking $PR_FILE against $BASE_FILE"
            
            if [ -f "$BASE_FILE" ]; then
              FOUND_FILES=true
              echo "Found matching file pair to compare"
              # Compare the benchmarks
              if ! scripts/tools/compare_benchmarks.sh "$PR_FILE" "$BASE_FILE" $WARNING_THRESHOLD $FAILURE_THRESHOLD; then
                SUCCESS=false
              fi
            else
              echo "⚠️ Could not find matching base file: $BASE_FILE"
            fi
          done
          
          if [ "$FOUND_FILES" = false ]; then
            echo "::warning::No matching benchmark files were found to compare!"
          fi
          
          # Check if we found any critical regressions
          if [ "$SUCCESS" = false ]; then
            echo "::error::Critical performance regressions detected! Review benchmark results."
            exit 1
          fi
      
      - name: Generate summary report
        if: success() || failure()
        run: |
          echo "# Benchmark Comparison Summary" > $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Comparing performance between PR and base branch" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Benchmark | Base | PR | Change |" >> $GITHUB_STEP_SUMMARY
          echo "|-----------|------|-------|-------|" >> $GITHUB_STEP_SUMMARY
          
          # Add each benchmark result to the summary
          PR_RESULTS=$(find target/criterion -name "new.json" | sort)
          for PR_FILE in $PR_RESULTS; do
            # Extract benchmark name
            BENCH_NAME=$(basename $(dirname "$PR_FILE"))
            
            # Find corresponding base file
            RELATIVE_PATH=${PR_FILE#"target/criterion/"}
            BASE_FILE="target/criterion/${RELATIVE_PATH/new/base}"
            
            if [ -f "$BASE_FILE" ]; then
              PR_TIME=$(jq '.mean.point_estimate' "$PR_FILE")
              BASE_TIME=$(jq '.mean.point_estimate' "$BASE_FILE")
              
              if [ -n "$PR_TIME" ] && [ -n "$BASE_TIME" ] && [ "$BASE_TIME" != "null" ] && [ "$PR_TIME" != "null" ]; then
                PERCENT_CHANGE=$(echo "scale=2; 100 * ($PR_TIME - $BASE_TIME) / $BASE_TIME" | bc)
                
                # Format the change cell with color indicators
                CHANGE_CELL="$PERCENT_CHANGE%"
                if (( $(echo "$PERCENT_CHANGE >= $FAILURE_THRESHOLD" | bc -l) )); then
                  CHANGE_CELL="⛔ $PERCENT_CHANGE% slower"
                elif (( $(echo "$PERCENT_CHANGE >= $WARNING_THRESHOLD" | bc -l) )); then
                  CHANGE_CELL="⚠️ $PERCENT_CHANGE% slower"
                elif (( $(echo "$PERCENT_CHANGE <= -$WARNING_THRESHOLD" | bc -l) )); then
                  CHANGE_CELL="✅ $PERCENT_CHANGE% faster"
                fi
                
                echo "| $BENCH_NAME | ${BASE_TIME%.*} ns | ${PR_TIME%.*} ns | $CHANGE_CELL |" >> $GITHUB_STEP_SUMMARY
              fi
            fi
          done
          
          # Add debug info if no results were found
          if [ -z "$PR_RESULTS" ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "⚠️ **No benchmark results were found to compare**" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Files in target/criterion:" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            find target/criterion -type f -name "*.json" | sort >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Thresholds: Warning at $WARNING_THRESHOLD%, Failure at $FAILURE_THRESHOLD%" >> $GITHUB_STEP_SUMMARY 